{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## LightGBM, XGBoost and CatBoost - Tutorial Notebook / Competition Kernel (also my Medium Blog post)\n\n<a href=\"https://bit.ly/2Z2PUXl\"><h1 style=\"font-size:250%; font-family:cursive; color:#ff6666;\"><b>Link to my Detailed YouTube Video Explaining the whole Notebook</b></h1></a>\n\n\n[![IMAGE ALT TEXT](https://imgur.com/U4WssYM.png)](https://bit.ly/2Z2PUXl \"Predict Transaction Value of a Bank's Potential Customers - Kaggle Santander Value Prediction Competition\")\n\n\nGradient Boosted trees have become one of the most powerful algorithms for training on tabular data. Over the recent past, we’ve been fortunate to have may implementations of boosted trees – each with their own unique characteristics.\nIn this notebook, I will implement LightGBM, XGBoost and CatBoost to tackle this Kaggle problem.\n\n**What is  Boosting**\n\nTo understand the absolute basics of the need for Boosting algorithm, lets ask a basic question - If a data point is incorrectly predicted by our first model, and then the next (probably all models), will combining the predictions provide better results? Such questions are handled by boosting algorithm.\n\nSo, Boosting is a sequential technique which works on the principle of an ensemble, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model.\n\nThe basic principle behind the working of the boosting algorithm is to generate multiple weak learners and combine their predictions to form one strong rule. These weak rules are generated by applying base Machine Learning algorithms on different distributions of the data set. These algorithms generate weak rules for each iteration. After multiple iterations, the weak learners are combined to form a strong learner that will predict a more accurate outcome.\n Note that a weak learner is one which is slightly better than random guessing. For example, a decision tree whose predictions are slightly better than 50%.\n\n **Gradient Boosting** works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor.\n\n**Here’s how the algorithm works**:\n\n**Step 1**: The base algorithm reads the data and assigns equal weight to each sample observation.\n\n**Step 2**: False predictions made by the base learner are identified. In the next iteration, these false predictions are assigned to the next base learner with a higher weightage on these incorrect predictions.\n\n**Step 3**: Repeat step 2 until the algorithm can correctly classify the output.\n\nTherefore, the main aim of Boosting is to focus more on miss-classified predictions.\n\n![img](https://i.imgur.com/OpP7D0X.png)\n\n[Source](https://catboost.ai/news/catboost-enables-fast-gradient-boosting-on-decision-trees-using-gpus)\n\nThese techniques are used to build ensemble models in an iterative way. On the first iteration, the algorithm learns the first tree to reduce the training error, shown on left-hand image above. The right-hand image above, shows the second iteration, in which the algorithm learns one more tree to reduce the error made by the first tree. The algorithm repeats this procedure until it builds a decent quality mode.\n\n![img](https://i.imgur.com/OpP7D0X.png)\n\nThe common approach for classification uses Logloss while regression optimizes using root mean square error. Ranking tasks commonly implements some variation of LambdaRank.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import preprocessing, model_selection, metrics\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\n\nfrom IPython.display import display # Allows the use of display() for DataFrames\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-04-19T10:22:47.783908Z","iopub.execute_input":"2022-04-19T10:22:47.784257Z","iopub.status.idle":"2022-04-19T10:22:50.909434Z","shell.execute_reply.started":"2022-04-19T10:22:47.784224Z","shell.execute_reply":"2022-04-19T10:22:50.908645Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-04-19T10:22:50.913473Z","iopub.execute_input":"2022-04-19T10:22:50.913742Z","iopub.status.idle":"2022-04-19T10:22:51.618877Z","shell.execute_reply.started":"2022-04-19T10:22:50.913715Z","shell.execute_reply":"2022-04-19T10:22:51.618104Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Tue Apr 19 10:22:51 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   41C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/santander-value-prediction-challenge/train.csv')\n\n# Given the test.csv file is huge, reading which each time during development\n# takes couple of minutes - making my development process slower.\n# I am reading only first 100 rows during development.\n# test_df = pd.read_csv('../input/santander-value-prediction-challenge/test.csv', nrows=100)\n\n# But in Kaggle Kernel, and before final submission\n# comment-out the above line and un-comment below line to read the full train.csv\ntest_df = pd.read_csv('../input/santander-value-prediction-challenge/test.csv')\n\ntrain_df.head()","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:22:51.620513Z","iopub.execute_input":"2022-04-19T10:22:51.620885Z","iopub.status.idle":"2022-04-19T10:24:10.544545Z","shell.execute_reply.started":"2022-04-19T10:22:51.620843Z","shell.execute_reply":"2022-04-19T10:24:10.543544Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"          ID      target  48df886f9  0deb4b6a8  34b15f335  a8cb14b00  \\\n0  000d6aaf2  38000000.0        0.0          0        0.0          0   \n1  000fbd867    600000.0        0.0          0        0.0          0   \n2  0027d6b71  10000000.0        0.0          0        0.0          0   \n3  0028cbf45   2000000.0        0.0          0        0.0          0   \n4  002a68644  14400000.0        0.0          0        0.0          0   \n\n   2f0771a37  30347e683  d08d1fbe3  6ee66e115  ...  3ecc09859  9281abeea  \\\n0          0          0          0          0  ...        0.0        0.0   \n1          0          0          0          0  ...        0.0        0.0   \n2          0          0          0          0  ...        0.0        0.0   \n3          0          0          0          0  ...        0.0        0.0   \n4          0          0          0          0  ...        0.0        0.0   \n\n   8675bec0b  3a13ed79a  f677d4d13  71b203550  137efaa80  fb36b89d9  \\\n0        0.0          0          0          0          0          0   \n1        0.0          0          0          0          0          0   \n2        0.0          0          0          0          0          0   \n3        0.0          0          0          0          0          0   \n4        0.0          0          0          0          0          0   \n\n   7e293fbaf  9fc776466  \n0          0          0  \n1          0          0  \n2          0          0  \n3          0          0  \n4          0          0  \n\n[5 rows x 4993 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>target</th>\n      <th>48df886f9</th>\n      <th>0deb4b6a8</th>\n      <th>34b15f335</th>\n      <th>a8cb14b00</th>\n      <th>2f0771a37</th>\n      <th>30347e683</th>\n      <th>d08d1fbe3</th>\n      <th>6ee66e115</th>\n      <th>...</th>\n      <th>3ecc09859</th>\n      <th>9281abeea</th>\n      <th>8675bec0b</th>\n      <th>3a13ed79a</th>\n      <th>f677d4d13</th>\n      <th>71b203550</th>\n      <th>137efaa80</th>\n      <th>fb36b89d9</th>\n      <th>7e293fbaf</th>\n      <th>9fc776466</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000d6aaf2</td>\n      <td>38000000.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fbd867</td>\n      <td>600000.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0027d6b71</td>\n      <td>10000000.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0028cbf45</td>\n      <td>2000000.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>002a68644</td>\n      <td>14400000.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 4993 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_df.head()","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:24:10.546406Z","iopub.execute_input":"2022-04-19T10:24:10.546819Z","iopub.status.idle":"2022-04-19T10:24:10.576676Z","shell.execute_reply.started":"2022-04-19T10:24:10.546774Z","shell.execute_reply":"2022-04-19T10:24:10.575765Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"          ID  48df886f9  0deb4b6a8  34b15f335  a8cb14b00  2f0771a37  \\\n0  000137c73        0.0        0.0        0.0        0.0        0.0   \n1  00021489f        0.0        0.0        0.0        0.0        0.0   \n2  0004d7953        0.0        0.0        0.0        0.0        0.0   \n3  00056a333        0.0        0.0        0.0        0.0        0.0   \n4  00056d8eb        0.0        0.0        0.0        0.0        0.0   \n\n   30347e683  d08d1fbe3  6ee66e115  20aa07010  ...  3ecc09859  9281abeea  \\\n0        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n1        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n2        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n3        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n4        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n\n   8675bec0b  3a13ed79a  f677d4d13  71b203550  137efaa80  fb36b89d9  \\\n0        0.0        0.0        0.0        0.0        0.0        0.0   \n1        0.0        0.0        0.0        0.0        0.0        0.0   \n2        0.0        0.0        0.0        0.0        0.0        0.0   \n3        0.0        0.0        0.0        0.0        0.0        0.0   \n4        0.0        0.0        0.0        0.0        0.0        0.0   \n\n   7e293fbaf  9fc776466  \n0        0.0        0.0  \n1        0.0        0.0  \n2        0.0        0.0  \n3        0.0        0.0  \n4        0.0        0.0  \n\n[5 rows x 4992 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>48df886f9</th>\n      <th>0deb4b6a8</th>\n      <th>34b15f335</th>\n      <th>a8cb14b00</th>\n      <th>2f0771a37</th>\n      <th>30347e683</th>\n      <th>d08d1fbe3</th>\n      <th>6ee66e115</th>\n      <th>20aa07010</th>\n      <th>...</th>\n      <th>3ecc09859</th>\n      <th>9281abeea</th>\n      <th>8675bec0b</th>\n      <th>3a13ed79a</th>\n      <th>f677d4d13</th>\n      <th>71b203550</th>\n      <th>137efaa80</th>\n      <th>fb36b89d9</th>\n      <th>7e293fbaf</th>\n      <th>9fc776466</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000137c73</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00021489f</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0004d7953</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00056a333</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00056d8eb</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 4992 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df.info()","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:24:10.580210Z","iopub.execute_input":"2022-04-19T10:24:10.580519Z","iopub.status.idle":"2022-04-19T10:24:10.760553Z","shell.execute_reply.started":"2022-04-19T10:24:10.580490Z","shell.execute_reply":"2022-04-19T10:24:10.759784Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4459 entries, 0 to 4458\nColumns: 4993 entries, ID to 9fc776466\ndtypes: float64(1845), int64(3147), object(1)\nmemory usage: 169.9+ MB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Initial Observations looking at the above data\n\n- Column name does not mean anything now, as they are all anonymized\n- The dataframe is full of zero values.\n- The dataset is a sparse tabular one refer [this](https://www.kaggle.com/c/santander-value-prediction-challenge/discussion/59128)\n\nTarget Variable:\n\nFirst doing some scatter plot of the target variable to check for visible outliers.","metadata":{}},{"cell_type":"code","source":"print('Train rows and columns: ', train_df.shape)\n\n# Keeping below line commented out as its huge 49,342 row file with 1gb size and so take longer to run each time\nprint('Test rows and columns: ', test_df.shape)","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:24:10.763577Z","iopub.execute_input":"2022-04-19T10:24:10.763929Z","iopub.status.idle":"2022-04-19T10:24:10.776855Z","shell.execute_reply.started":"2022-04-19T10:24:10.763895Z","shell.execute_reply":"2022-04-19T10:24:10.776097Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Train rows and columns:  (4459, 4993)\nTest rows and columns:  (49342, 4992)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Keeping below lines commented out during development\n\nplt.figure(figsize=(8,6))\nplt.scatter(range(train_df.shape[0]), np.sort(train_df['target'].values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('Target', fontsize=12)\nplt.title('Distribution of Target', fontsize=14)\nplt.show()","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:24:10.778055Z","iopub.execute_input":"2022-04-19T10:24:10.778415Z","iopub.status.idle":"2022-04-19T10:24:11.103512Z","shell.execute_reply.started":"2022-04-19T10:24:10.778380Z","shell.execute_reply":"2022-04-19T10:24:11.102403Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 576x432 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"TO-DO - So there's not too much of outliers (visibly) but the distribution range is high. Now want to do a histogram","metadata":{}},{"cell_type":"markdown","source":"## Checking for missing / null values in data","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"print(\"All Features in Train data with NaN Values =\", str(train_df.columns[train_df.isnull().sum() != 0].size) )\n# print(\"All Features in Test data with NaN Values =\", str(test_df.columns[train_df.isnull().sum() != 0].size) )","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:24:11.105114Z","iopub.execute_input":"2022-04-19T10:24:11.105786Z","iopub.status.idle":"2022-04-19T10:24:11.179368Z","shell.execute_reply.started":"2022-04-19T10:24:11.105735Z","shell.execute_reply":"2022-04-19T10:24:11.178089Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"All Features in Train data with NaN Values = 0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Remove constant columns from data","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"const_columns_to_remove = []\nfor col in train_df.columns:\n    if col != 'ID' and col != 'target':\n        if train_df[col].std() == 0:\n            const_columns_to_remove.append(col)\n\n# Now remove that array of const columns from the data\ntrain_df.drop(const_columns_to_remove, axis=1, inplace=True)\ntest_df.drop(const_columns_to_remove, axis=1, inplace=True)\n\n# Print to see the reduction of columns\nprint('train_df rows and columns after removing constant columns: ', train_df.shape)\n\nprint('Following `{}` Constant Column\\n are removed'.format(len(const_columns_to_remove)))\nprint(const_columns_to_remove)","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:24:11.181379Z","iopub.execute_input":"2022-04-19T10:24:11.181795Z","iopub.status.idle":"2022-04-19T10:24:12.093137Z","shell.execute_reply.started":"2022-04-19T10:24:11.181751Z","shell.execute_reply":"2022-04-19T10:24:12.092064Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"train_df rows and columns after removing constant columns:  (4459, 4737)\nFollowing `256` Constant Column\n are removed\n['d5308d8bc', 'c330f1a67', 'eeac16933', '7df8788e8', '5b91580ee', '6f29fbbc7', '46dafc868', 'ae41a98b6', 'f416800e9', '6d07828ca', '7ac332a1d', '70ee7950a', '833b35a7c', '2f9969eab', '8b1372217', '68322788b', '2288ac1a6', 'dc7f76962', '467044c26', '39ebfbfd9', '9a5ff8c23', 'f6fac27c8', '664e2800e', 'ae28689a2', 'd87dcac58', '4065efbb6', 'f944d9d43', 'c2c4491d5', 'a4346e2e2', '1af366d4f', 'cfff5b7c8', 'da215e99e', '5acd26139', '9be9c6cef', '1210d0271', '21b0a54cb', 'da35e792b', '754c502dd', '0b346adbd', '0f196b049', 'b603ed95d', '2a50e001c', '1e81432e7', '10350ea43', '3c7c7e24c', '7585fce2a', '64d036163', 'f25d9935c', 'd98484125', '95c85e227', '9a5273600', '746cdb817', '6377a6293', '7d944fb0c', '87eb21c50', '5ea313a8c', '0987a65a1', '2fb7c2443', 'f5dde409b', '1ae50d4c3', '2b21cd7d8', '0db8a9272', '804d8b55b', '76f135fa6', '7d7182143', 'f88e61ae6', '378ed28e0', 'ca4ba131e', '1352ddae5', '2b601ad67', '6e42ff7c7', '22196a84c', '0e410eb3d', '992e6d1d3', '90a742107', '08b9ec4ae', 'd95203ded', '58ad51def', '9f69ae59f', '863de8a31', 'be10df47c', 'f006d9618', 'a7e39d23d', '5ed0abe85', '6c578fe94', '7fa4fcee9', '5e0571f07', 'fd5659511', 'e06b9f40f', 'c506599c8', '99de8c2dc', 'b05f4b229', '5e0834175', 'eb1cc0d9c', 'b281a62b9', '00fcf67e4', 'e37b65992', '2308e2b29', 'c342e8709', '708471ebf', 'f614aac15', '15ecf7b68', '3bfe540f1', '7a0d98f3c', 'e642315a5', 'c16d456a7', '0c9b5bcfa', 'b778ab129', '2ace87cdd', '697a566f0', '97b1f84fc', '34eff114b', '5281333d7', 'c89f3ba7e', 'cd6d3c7e6', 'fc7c8f2e8', 'abbbf9f82', '24a233e8f', '8e26b560e', 'a28ac1049', '504502ce1', 'd9a8615f3', '4efd6d283', '34cc56e83', '93e98252a', '2b6cef19e', 'c7f70a49b', '0d29ab7eb', 'e4a0d39b7', 'a4d1a8409', 'bc694fc8f', '3a36fc3a2', '4ffba44d3', '9bfdec4bc', '66a866d2f', 'f941e9df7', 'e7af4dbf3', 'dc9a54a3e', '748168a04', 'bba8ce4bb', 'ff6f62aa4', 'b06fe66ba', 'ae87ebc42', 'f26589e57', '963bb53b1', 'a531a4bf0', '9fc79985d', '9350d55c1', 'de06e884c', 'fc10bdf18', 'e0907e883', 'c586d79a1', 'e15e1513d', 'a06067897', '643e42fcb', '217cd3838', '047ebc242', '9b6ce40cf', '3b2c972b3', '17a7bf25a', 'c9028d46b', '9e0473c91', '6b041d374', '783c50218', '19122191d', 'ce573744f', '1c4ea481e', 'fbd6e0a0b', '69831c049', 'b87e3036b', '54ba515ee', 'a09ba0b15', '90f77ec55', 'fb02ef0ea', '3b0cccd29', 'fe9ed417c', '589e8bd6f', '17b5a03fd', '80e16b49a', 'a3d5c2c2a', '1bd3a4e92', '611d81daa', '3d7780b1c', '113fd0206', '5e5894826', 'cb36204f9', 'bc4e3d600', 'c66e2deb0', 'c25851298', 'a7f6de992', '3f93a3272', 'c1b95c2ec', '6bda21fee', '4a64e56e7', '943743753', '20854f8bf', 'ac2e428a9', '5ee7de0be', '316423a21', '2e52b0c6a', '8bdf6bc7e', '8f523faf2', '4758340d5', '8411096ec', '9678b95b7', 'a185e35cc', 'fa980a778', 'c8d90f7d7', '080540c81', '32591c8b4', '5779da33c', 'bb425b41e', '01599af81', '1654ab770', 'd334a588e', 'b4353599c', '51b53eaec', '2cc0fbc52', '45ffef194', 'c15ac04ee', '5b055c8ea', 'd0466eb58', 'a80633823', 'a117a5409', '7ddac276f', '8c32df8b3', 'e5649663e', '6c16efbb8', '9118fd5ca', 'ca8d565f1', '16a5bb8d2', 'fd6347461', 'f5179fb9c', '97428b646', 'f684b0a96', 'e4b2caa9f', '2c2d9f267', '96eb14eaf', 'cb2cb460c', '86f843927', 'ecd16fc60', '801c6dc8e', 'f859a25b8', 'ae846f332', '2252c7403', 'fb9e07326', 'd196ca1fd', 'a8e562e8e', 'eb6bb7ce1', '5beff147e', '52b347cdc', '4600aadcf', '6fa0b9dab', '43d70cc4d', '408021ef8', 'e29d22b59']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Remove Duplicate Columns\n\n**I will be using the duplicated() function of pandas - here's how it works:**\n\nSuppose the columns of the data frame are `['alpha','beta','alpha']`\n\n`df.columns.duplicated()` returns a boolean array: a `True` or `False` for each column. If it is `False` then the column name is unique up to that point, if it is `True` then the column name is duplicated earlier. For example, using the given example, the returned value would be `[False,False,True]`. \n\n`Pandas` allows one to index using boolean values whereby it selects only the `True` values. Since we want to keep the unduplicated columns, we need the above boolean array to be flipped (ie `[True, True, False] = ~[False,False,True]`)\n\nFinally, `df.loc[:,[True,True,False]]` selects only the non-duplicated columns using the aforementioned indexing capability. \n\n**Note**: the above only checks columns names, *not* column values.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"train_df = train_df.loc[:,~train_df.columns.duplicated()]\nprint('Train rows and columns after removing duplicate columns: ', train_df.shape)","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:24:12.094837Z","iopub.execute_input":"2022-04-19T10:24:12.095251Z","iopub.status.idle":"2022-04-19T10:24:12.165285Z","shell.execute_reply.started":"2022-04-19T10:24:12.095206Z","shell.execute_reply":"2022-04-19T10:24:12.164331Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Train rows and columns after removing duplicate columns:  (4459, 4737)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Handling Sparse data\n\n**What is Sparse data**\n\nAs an example, let's say that we are collecting data from a device which has 12 sensors. And you have collected data for 10 days.\n\nThe data you have collected is as follows:\n\n[![enter image description here][1]][1]\n\n\nThe above is an example of sparse data because most of the sensor outputs are zero. Which means those sensors are functioning properly but the actual reading is zero. Although this matrix has high dimensional data (12 axises) it can be said that it contains less information.\n\nSo basically, sparse data means that there are many gaps present in the data being recorded. For example, in the case of the sensor mentioned above, the sensor may send a signal only when the state changes, like when there is a movement of the door in a room. This data will be obtained intermittently because the door is not always moving. Hence, this is sparse data.\n\n  [1]: https://i.stack.imgur.com/Af5IH.png\n\n\nFirst lets have a look at or train_df data again, that how much of sparse data is there. And as we can see there are plenty of '0'","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"train_df.head()","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:24:12.166792Z","iopub.execute_input":"2022-04-19T10:24:12.168283Z","iopub.status.idle":"2022-04-19T10:24:12.209788Z","shell.execute_reply.started":"2022-04-19T10:24:12.168176Z","shell.execute_reply":"2022-04-19T10:24:12.208915Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"          ID      target  48df886f9  0deb4b6a8  34b15f335  a8cb14b00  \\\n0  000d6aaf2  38000000.0        0.0          0        0.0          0   \n1  000fbd867    600000.0        0.0          0        0.0          0   \n2  0027d6b71  10000000.0        0.0          0        0.0          0   \n3  0028cbf45   2000000.0        0.0          0        0.0          0   \n4  002a68644  14400000.0        0.0          0        0.0          0   \n\n   2f0771a37  30347e683  d08d1fbe3  6ee66e115  ...  3ecc09859  9281abeea  \\\n0          0          0          0          0  ...        0.0        0.0   \n1          0          0          0          0  ...        0.0        0.0   \n2          0          0          0          0  ...        0.0        0.0   \n3          0          0          0          0  ...        0.0        0.0   \n4          0          0          0          0  ...        0.0        0.0   \n\n   8675bec0b  3a13ed79a  f677d4d13  71b203550  137efaa80  fb36b89d9  \\\n0        0.0          0          0          0          0          0   \n1        0.0          0          0          0          0          0   \n2        0.0          0          0          0          0          0   \n3        0.0          0          0          0          0          0   \n4        0.0          0          0          0          0          0   \n\n   7e293fbaf  9fc776466  \n0          0          0  \n1          0          0  \n2          0          0  \n3          0          0  \n4          0          0  \n\n[5 rows x 4737 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>target</th>\n      <th>48df886f9</th>\n      <th>0deb4b6a8</th>\n      <th>34b15f335</th>\n      <th>a8cb14b00</th>\n      <th>2f0771a37</th>\n      <th>30347e683</th>\n      <th>d08d1fbe3</th>\n      <th>6ee66e115</th>\n      <th>...</th>\n      <th>3ecc09859</th>\n      <th>9281abeea</th>\n      <th>8675bec0b</th>\n      <th>3a13ed79a</th>\n      <th>f677d4d13</th>\n      <th>71b203550</th>\n      <th>137efaa80</th>\n      <th>fb36b89d9</th>\n      <th>7e293fbaf</th>\n      <th>9fc776466</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000d6aaf2</td>\n      <td>38000000.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fbd867</td>\n      <td>600000.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0027d6b71</td>\n      <td>10000000.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0028cbf45</td>\n      <td>2000000.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>002a68644</td>\n      <td>14400000.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 4737 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Check and handle total memory of data\n\n`get_dummies` pandas function converts categorical variables into indicator variables.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def print_memory_usage_of_df(df):\n    bytes_per_mb = 0.000001\n    memory_usage = round(df.memory_usage().sum() * bytes_per_mb, 3)\n    print('Memory usage is ', str(memory_usage) + \" MB\")\n\nprint_memory_usage_of_df(train_df)\nprint(train_df.shape)","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:24:12.211165Z","iopub.execute_input":"2022-04-19T10:24:12.211739Z","iopub.status.idle":"2022-04-19T10:24:12.507057Z","shell.execute_reply.started":"2022-04-19T10:24:12.211690Z","shell.execute_reply":"2022-04-19T10:24:12.505592Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Memory usage is  168.978 MB\n(4459, 4737)\n","output_type":"stream"}]},{"cell_type":"code","source":"dummy_encoded_train_df = pd.get_dummies(train_df)\ndummy_encoded_train_df.shape","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:24:12.511219Z","iopub.execute_input":"2022-04-19T10:24:12.512814Z","iopub.status.idle":"2022-04-19T10:24:13.805646Z","shell.execute_reply.started":"2022-04-19T10:24:12.512772Z","shell.execute_reply":"2022-04-19T10:24:13.804739Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(4459, 9195)"},"metadata":{}}]},{"cell_type":"code","source":"print_memory_usage_of_df(dummy_encoded_train_df)","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:24:13.807135Z","iopub.execute_input":"2022-04-19T10:24:13.807523Z","iopub.status.idle":"2022-04-19T10:24:14.221312Z","shell.execute_reply.started":"2022-04-19T10:24:13.807494Z","shell.execute_reply":"2022-04-19T10:24:14.220414Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Memory usage is  188.825 MB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We see that the memory usage of the dummy_encoded_train_df data  frame is larger compared to the original, because now the number of columns have increased in the data frame.\n\n##### So lets apply `sparse=True` if it reduces the memory-usages to some extent.\n\nThis parameter `sparse` defaults to False. If True the encoded columns are returned as **SparseArray**. By setting `sparse=True` we create a sparse data frame directly","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"dummy_encoded_sparse_train_df = pd.get_dummies(train_df, sparse=True)\ndummy_encoded_sparse_train_df.shape","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:24:14.222626Z","iopub.execute_input":"2022-04-19T10:24:14.223147Z","iopub.status.idle":"2022-04-19T10:24:15.456722Z","shell.execute_reply.started":"2022-04-19T10:24:14.223105Z","shell.execute_reply":"2022-04-19T10:24:15.455801Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(4459, 9195)"},"metadata":{}}]},{"cell_type":"code","source":"print_memory_usage_of_df(dummy_encoded_sparse_train_df)\n","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:24:15.458062Z","iopub.execute_input":"2022-04-19T10:24:15.458459Z","iopub.status.idle":"2022-04-19T10:24:15.899565Z","shell.execute_reply.started":"2022-04-19T10:24:15.458419Z","shell.execute_reply":"2022-04-19T10:24:15.898649Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Memory usage is  168.965 MB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"But looks like in this case the reduction in memory_size was not a huge amount. So lets try some other alternative\n\n## [Pandas Sparse Structures](https://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html#sparse-data-structures)\n\nPandas provides data structures for efficient storage of sparse data. In these structures, zero values (or any other specified value) are not actually stored in the array. Rather, you can view these objects as being “compressed” where any data matching a specific value (NaN / missing value, though any value can be chosen, including 0) is omitted. The compressed values are not actually stored in the array.\n\nStoring only the non-zero values and their positions is a common technique in storing sparse data sets.\n\nThis hugely reduces the memory usage of our data set and “compress” the data frame.\n\nIn our example, we will convert the one-hot encoded columns into SparseArrays, which are 1-d arrays where only non-zero values are stored.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def convert_df_to_sparse_array(df, exclude_columns=[]):\n    df = df.copy()\n    exclude_columns = set(exclude_columns)\n\n    for (column_name, column_data) in df.iteritems():\n        if column_name in exclude_columns:\n            continue\n        df[column_name] = pd.SparseArray(column_data.values, dtype='uint8')\n\n    return df\n\n# Now convert our earlier dummy_encoded_train_df with above function and check memory_size\n\n# train_data_post_conversion_to_sparse_array = convert_df_to_sparse_array(dummy_encoded_train_df)\n# print('Sparse Array Train_DF rows and columns: ', train_data_post_conversion_to_sparse_array.shape)\n# print_memory_usage_of_df(train_data_post_conversion_to_sparse_array)\n\n# Commenting the above out - for running the Notebook faster during my development \n# Because df.iteritems() will take a huge time to process the data - see warning below","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:24:15.901052Z","iopub.execute_input":"2022-04-19T10:24:15.901669Z","iopub.status.idle":"2022-04-19T10:24:15.908815Z","shell.execute_reply.started":"2022-04-19T10:24:15.901607Z","shell.execute_reply":"2022-04-19T10:24:15.907933Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"**We see the that the memory_usage is substantially reduced now**\n\n### A warning on using df.iteritems()\n\nThe df.iteritems() iterates over columns and not rows. Generally iteration over dataframes is an anti-pattern, and something we should avoid, unless you want to get used to a lot of waiting.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"## Sparse Data Removal (Following simpler plain-vanilla technique)\n\n### For this notebook, I will go with the easier approach to handle sparse data - which is just to drop it from the dataframe\n\nlike below code, I will do this for the sake of running this notebook faster for now\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def drop_sparse_from_train_test(train, test):\n    column_list_to_drop_data_from = [x for x in train.columns if not x in ['ID','target']]\n    for f in column_list_to_drop_data_from:\n        if len(np.unique(train[f]))<2:\n            train.drop(f, axis=1, inplace=True)\n            test.drop(f, axis=1, inplace=True)\n    return train, test\n\ntrain_df, test_df = drop_sparse_from_train_test(train_df, test_df)\n","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:24:15.910265Z","iopub.execute_input":"2022-04-19T10:24:15.910701Z","iopub.status.idle":"2022-04-19T10:24:16.297174Z","shell.execute_reply.started":"2022-04-19T10:24:15.910657Z","shell.execute_reply":"2022-04-19T10:24:16.296399Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Split data into Train and Test for Model Training","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"X_train = train_df.drop(['ID', 'target'], axis=1)\n\ny_train = np.log1p(train_df['target'].values)\n\nX_test_original = test_df.drop('ID', axis=1)\n\nX_train_split, X_validation, y_train_split, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=42)","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:24:16.298539Z","iopub.execute_input":"2022-04-19T10:24:16.298889Z","iopub.status.idle":"2022-04-19T10:24:16.931614Z","shell.execute_reply.started":"2022-04-19T10:24:16.298852Z","shell.execute_reply":"2022-04-19T10:24:16.930850Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## LightGBM Model Training\n\n### Fundamentals of LightGBM Model\n\nIt is a **gradient boosting** model that makes use of tree based learning algorithms. It is considered to be a fast processing algorithm.\n\nWhile other algorithms trees grow horizontally, LightGBM algorithm grows vertically, meaning it grows leaf-wise and other algorithms grow level-wise. LightGBM chooses the leaf with large loss to grow. It can lower down more loss than a level wise algorithm when growing the same leaf.\n\n![img](https://i.imgur.com/pzOP2Lb.png)\n\n[Source of Image](https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/)\n\nLight GBM is prefixed as Light because of its high speed. Light GBM can handle the large size of data and takes lower memory to run.\n\nAnother reason why Light GBM is so popular is because it focuses on accuracy of results. LGBM also supports GPU learning and thus data scientists are widely using LGBM for data science application development.\n\n**Leaf growth technique in LightGBM**\n\nLightGBM uses leaf-wise (best-first) tree growth. It chooses to grow the leaf that minimizes the loss, allowing a growth of an imbalanced tree. Because it doesn’t grow level-wise, but leaf-wise, over-fitting can happen when data is small. In these cases, it is important to control the tree depth.\n\n### When to use LightGBM ?\n\nLightGBM is not preferred for a small volume of datasets as it can easily overfit small data due to its sensitivity. Hence, it generally advised for data having more than 10,000+ rows, though there is no fixed threshold that helps in deciding the usage of LightGBM.\n\n## What are LightGBM Parameters?\n\nWhile, LightGBM has more than 100 parameters that are given in the [documentation of LightGBM](https://github.com/microsoft/LightGBM), let's checkout the most important ones.\n\n#### Control Parameters\n\n**Max depth**: It gives the depth of the tree and also controls the overfitting of the model. If you feel your model is getting overfitted lower down the max depth.\n\n**Min_data_in_leaf**: Leaf minimum number of records also used for controlling overfitting of the model.\n\n**Feature_fraction**: It decides the randomly chosen parameter in every iteration for building trees. If it is 0.7 then it means 70% of the parameter would be used.\n\n**Bagging_fraction**: It checks for the data fraction that will be used in every iteration. Often, used to increase the training speed and avoid overfitting.\n\n**Early_stopping_round**: If the metric of the validation data does show any improvement in last early_stopping_round rounds. It will lower the imprudent iterations.\n\n**Lambda**: It states regularization. Its values range from 0 to 1.\n\n**Min_gain_to_split**: Used to control the number of splits in the tree.\n\n### Core Parameters\n\n**Task**: It tells about the task that is to be performed on the data. It can either train on the data or prediction on the data.\n\n**Application**: This parameter specifies whether to do regression or classification. LightGBM default parameter for application is regression.\n\n**Binary**: It is used for binary classification.\n\n**Multiclass**: It is used for multiclass classification problems.\n\n**Regression**: It is used for doing regression.\n\n**Boosting**: It specifies the algorithm type.\n\n**rf** :  Used for Random Forest.\n\n**Goss**: Gradient-based One Side Sampling.\n\n**Num_boost_round**: It tells about the boosting iterations.\n\n**Learning_rate**: The role of learning rate is to power the magnitude of the changes in the approximate that gets updated from each tree’s output. It determines the contribution of each tree on the final outcome and controls how quickly the algorithm proceeds down the gradient descent (learns); Typical values between 0.001–0.3. Smaller values make the model robust to the specific characteristics of each individual tree, thus allowing it to generalize well. Smaller values also make it easier to stop prior to overfitting; however, they increase the risk of not reaching the optimum with a fixed number of trees and are more computationally demanding. This hyperparameter is also called shrinkage. Generally, the smaller this value, the more accurate the model can be but also will require more trees in the sequence.\n\n**Num_leaves**: It gives the total number of leaves that would be present in a full tree, default value: 31\n\n### Metric Parameter\n\nIt takes care of the loss while building the model. Some of them are stated below for classification as well as regression.\n\n**Mae**: Mean absolute error.\n\n**Mse**: Mean squared error.\n\n**Binary_logloss**: Binary Classification loss.\n\n**Multi_logloss**: Multi Classification loss.","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def light_gbm_model_run(train_x, train_y, validation_x, validation_y, test_x):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 100,\n        \"learning_rate\" : 0.001,\n        \"bagging_fraction\" : 0.6,\n        \"feature_fraction\" : 0.6,\n        \"bagging_frequency\" : 6,\n        \"bagging_seed\" : 42,\n        \"verbosity\" : -1,\n        \"seed\": 42\n    }\n    # Given its a regression case, I am using the RMSE as the metric.\n\n    lg_train = lgb.Dataset(train_x, label=train_y)\n    lg_validation = lgb.Dataset(validation_x, label=validation_y)\n    evals_result_lgbm = {}\n\n    model_light_gbm = lgb.train(params, lg_train, 5000,\n                      valid_sets=[lg_train, lg_validation],\n                      early_stopping_rounds=100,\n                      verbose_eval=150,\n                      evals_result=evals_result_lgbm )\n\n    pred_test_light_gbm = np.expm1(model_light_gbm.predict(test_x, num_iteration=model_light_gbm.best_iteration ))\n\n    return pred_test_light_gbm, model_light_gbm, evals_result_lgbm","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:24:16.933115Z","iopub.execute_input":"2022-04-19T10:24:16.933622Z","iopub.status.idle":"2022-04-19T10:24:16.943280Z","shell.execute_reply.started":"2022-04-19T10:24:16.933582Z","shell.execute_reply":"2022-04-19T10:24:16.942438Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Training and output of LightGBM Model\npredictions_test_y_light_gbm, model_lgbm, evals_result = light_gbm_model_run(X_train_split, y_train_split, X_validation, y_validation, X_test_original)\nprint('Output of LightGBM Model training..')","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:24:16.944751Z","iopub.execute_input":"2022-04-19T10:24:16.945099Z","iopub.status.idle":"2022-04-19T10:32:28.901152Z","shell.execute_reply.started":"2022-04-19T10:24:16.945062Z","shell.execute_reply":"2022-04-19T10:32:28.900303Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Training until validation scores don't improve for 100 rounds\n[150]\ttraining's rmse: 1.66447\tvalid_1's rmse: 1.63996\n[300]\ttraining's rmse: 1.5765\tvalid_1's rmse: 1.5927\n[450]\ttraining's rmse: 1.49849\tvalid_1's rmse: 1.55466\n[600]\ttraining's rmse: 1.42919\tvalid_1's rmse: 1.52339\n[750]\ttraining's rmse: 1.36631\tvalid_1's rmse: 1.49837\n[900]\ttraining's rmse: 1.30931\tvalid_1's rmse: 1.47791\n[1050]\ttraining's rmse: 1.25734\tvalid_1's rmse: 1.46143\n[1200]\ttraining's rmse: 1.20984\tvalid_1's rmse: 1.44818\n[1350]\ttraining's rmse: 1.16678\tvalid_1's rmse: 1.43796\n[1500]\ttraining's rmse: 1.12698\tvalid_1's rmse: 1.42969\n[1650]\ttraining's rmse: 1.09049\tvalid_1's rmse: 1.42292\n[1800]\ttraining's rmse: 1.05661\tvalid_1's rmse: 1.41849\n[1950]\ttraining's rmse: 1.02528\tvalid_1's rmse: 1.41488\n[2100]\ttraining's rmse: 0.995869\tvalid_1's rmse: 1.41222\n[2250]\ttraining's rmse: 0.968211\tvalid_1's rmse: 1.40996\n[2400]\ttraining's rmse: 0.941985\tvalid_1's rmse: 1.40807\n[2550]\ttraining's rmse: 0.917269\tvalid_1's rmse: 1.40669\n[2700]\ttraining's rmse: 0.893978\tvalid_1's rmse: 1.40569\n[2850]\ttraining's rmse: 0.871822\tvalid_1's rmse: 1.40492\n[3000]\ttraining's rmse: 0.850995\tvalid_1's rmse: 1.40427\n[3150]\ttraining's rmse: 0.831253\tvalid_1's rmse: 1.40393\n[3300]\ttraining's rmse: 0.812591\tvalid_1's rmse: 1.40376\nEarly stopping, best iteration is:\n[3289]\ttraining's rmse: 0.813895\tvalid_1's rmse: 1.40374\nOutput of LightGBM Model training..\n","output_type":"stream"}]},{"cell_type":"markdown","source":"\n## Hyper-Parameter Tuning in LightGBM\n\nParameter Tuning is an important part that is usually done by data scientists to achieve a good accuracy, fast result and to deal with overfitting. Let us see quickly some of the parameter tuning you can do for better results.\n\n**num_leaves**: This parameter is responsible for the complexity of the model. I normally start by trying values in the range [10,100]. But if you have a solid heuristic to choose tree depth you can always use it and set num_leaves to 2^tree_depth - 1\n\n[LightGBM Documentation](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html) says in respect -\nThis is the main parameter to control the complexity of the tree model. Theoretically, we can set num_leaves = 2^(max_depth) to obtain the same number of leaves as depth-wise tree. However, this simple conversion is not good in practice. The reason is that a leaf-wise tree is typically much deeper than a depth-wise tree for a fixed number of leaves. Unconstrained depth can induce over-fitting. Thus, when trying to tune the num_leaves, we should let it be smaller than 2^(max_depth). For example, when the max_depth=7 the depth-wise tree can get good accuracy, but setting num_leaves to 127 may cause over-fitting, and setting it to 70 or 80 may get better accuracy than depth-wise.\n\n**Min_data_in_leaf**: Assigning bigger value to this parameter can result in underfitting of the model. Giving it a value of 100 or 1000 is sufficient for a large dataset.\n\n**Max_depth**: Controls the depth of the individual trees. Typical values range from a depth of 3–8 but it is not uncommon to see a tree depth of 1. Smaller depth trees are computationally efficient (but require more trees); however, higher depth trees allow the algorithm to capture unique interactions but also increase the risk of over-fitting. Larger training data sets are more tolerable to deeper trees.\n\n**num_iterations**: Num_iterations specifies the number of boosting iterations (trees to build). The more trees you build the more accurate your model can be at the cost of:\n    - Longer training time\n    - Higher chance of over-fitting\nSo typically start with a lower number of trees to build a baseline and increase it later when you want to squeeze the last % out of your model.\n\nIt is recommended to use smaller `learning_rate` with larger `num_iterations`. Also, we should use `early_stopping_rounds` if we go for higher `num_iterations` to stop your training when it is not learning anything useful.\n\n**early_stopping_rounds** - \"early stopping\" refers to stopping the training process if the model's performance on a given validation set does not improve for several consecutive iterations. This parameter will stop training if the validation metric is not improving after the last early stopping round. It should be defined in pair with a number of iterations. If we set it too large we increase the chance of over-fitting. **The rule of thumb is to have it at 10% of your `num_iterations`**.\n\n\n#### So for my above implementation of LightGBM, initially for two of the LightGBM parameters as below got me a score of 1.47953 (in Kaggle Public Board)\n\n```\n\"num_leaves\" : 40,\n\"learning_rate\" : 0.004,\n```\n\nAnd now if I only tune these parameters as below\n\n```\n\"num_leaves\" : 100,\n\"learning_rate\" : 0.001,\n```\n#### I got my score very very slightly updated to 1.4714 (in Kaggle Public Board)\n\nI also tried the below one (keeping 'num_leaves' at 70 to avoid over-fitting)\n\n```\n\"num_leaves\" : 70,\n\"learning_rate\" : 0.001,\n```\nWith this - I got a score of 1.47234 (in Kaggle Public Board)\n\n## Features Importance in LightGBM","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"gain_light_gbm = model_lgbm.feature_importance('gain')\nfeature_imp_light_gbm = pd.DataFrame({'feature': model_lgbm.feature_name(),\n                                      'split': model_lgbm.feature_importance('split'),\n                                      'gain': 100 * gain_light_gbm / gain_light_gbm.sum()\n}).sort_values('gain', ascending=False)\nprint(feature_imp_light_gbm[:50])","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:32:28.902713Z","iopub.execute_input":"2022-04-19T10:32:28.903120Z","iopub.status.idle":"2022-04-19T10:32:28.937967Z","shell.execute_reply.started":"2022-04-19T10:32:28.903082Z","shell.execute_reply":"2022-04-19T10:32:28.937276Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"        feature  split      gain\n4135  f190486d6   6623  6.950093\n2378  58e2e02e6   6012  4.948486\n3470  eeb9cd3aa   5474  4.035842\n2617  9fd594eec   4157  3.262016\n4025  15ace8c9f   5198  2.999663\n8     20aa07010   3628  1.903282\n3576  58232a6fb   3458  1.472419\n834   6eef030c1   3798  1.292199\n1459  b43a7cfd5   4241  1.255624\n2690  fb0f5dbfe   4482  1.118876\n3666  491b9ee45   2625  1.040685\n1484  024c577b9   2931  1.027777\n4348  1702b5bf0   2993  0.934598\n4190  f74e8f13d   3636  0.931987\n566   66ace2992   3053  0.899695\n4513  c47340d97   3209  0.888001\n3727  d6bb78916   3296  0.863508\n2082  58e056e12   3651  0.859513\n863   fc99f9426   2474  0.778488\n3816  adb64ff71   2664  0.749633\n4458  190db8488   3040  0.721582\n4033  5c6487af1   2454  0.703383\n3224  ced6a7e91   1666  0.667195\n3796  ed8ff54b5    737  0.643995\n2137  241f0f867   2467  0.641655\n537   26fc93eb7   2559  0.620171\n3872  2288333b4   1182  0.614189\n3891  50e4f96cf   1147  0.594101\n2619  fb387ea33    998  0.583353\n828   6786ea46d    806  0.559286\n1380  6cf7866c1    985  0.547168\n4346  e176a204a   2342  0.544077\n34    87ffda550   1353  0.531567\n2214  1931ccfdd   2020  0.517504\n853   bc70cbc26    973  0.505660\n4321  c5a231d81   2253  0.502023\n3010  703885424   1944  0.488684\n3784  70feb1494   1879  0.486723\n3474  324921c7b   2431  0.477780\n213   186b87c05    345  0.476618\n2934  91f701ba2   1983  0.439053\n3988  45f6d00da   1365  0.421545\n624   0c9462c08   1232  0.420413\n4022  62e59a501   1955  0.418382\n545   0572565c2   1652  0.395456\n1750  5f341a818    877  0.389509\n1850  5a1589f1a   1570  0.379379\n1712  2ec5b290f   2078  0.377675\n645   6619d81fc   1627  0.365498\n1360  1db387535   1774  0.365378\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## XGBoost Model Training\n\n### Note on XGBoost\n\nBelow we will be using **XGBoost** which is an advanced version of Gradient boosting method, it literally means eXtreme Gradient Boosting. XGBoost dominates structured or tabular datasets on classification and regression predictive modeling problems. The XGBoost library implements the [gradient boosting decision tree algorithm](https://en.wikipedia.org/wiki/Gradient_boosting).\n\nDifferent from the traditional gradient descent technique, gradient enhancement helps to predict the optimal gradient of the additional model. This technique can reduce the output error at each iteration.\n\nIn practice what we do in order to build the learner is to:\n\n- Start with single root (contains all the training examples)\n\n- Iterate over all features and values per feature, and evaluate each possible split loss reduction:\n\n- gain = loss(father instances) - (loss(left branch)+loss(right branch))\n\n- The gain for the best split must be positive (and > min_split_gain parameter), otherwise we must stop growing the branch.\n\n**Leaf growth**\n\nXGboost splits up to the specified max_depth hyperparameter and then starts pruning the tree backwards and removes splits beyond which there is no positive gain. It uses this approach since sometimes a split of no loss reduction may be followed by a split with loss reduction. XGBoost can also perform leaf-wise tree growth (as LightGBM).\n\nNormally it is impossible to enumerate all the possible tree structures q. A greedy algorithm that starts from a single leaf and iteratively adds branches to the tree is used instead. Assume that I_L and I_R are the instance sets of left and right nodes after the split. Then the loss reduction after the split is given by,\n\n![](https://i.imgur.com/jzyLh81.png)\n\n## Differences in LightGBM & XGBoost\n\nLightGBM uses a novel technique of Gradient-based One-Side Sampling (GOSS) to filter out the data instances for finding a split value while XGBoost uses pre-sorted algorithm & Histogram-based algorithm for computing the best split. Here instances mean observations/samples.\n\nLet's see how pre-sorting splitting works-\n\n- For each node, enumerate over all features\n\n- For each feature, sort the instances by feature value\n\n- Use a linear scan to decide the best split along that feature basis information gain\n\n- Take the best split solution along all the features\n\nIn simple terms, Histogram-based algorithm splits all the data points for a feature into discrete bins and uses these bins to find the split value of histogram. While, it is efficient than pre-sorted algorithm in training speed which enumerates all possible split points on the pre-sorted feature values, it is still behind GOSS in terms of speed.\n\n## XGBoost Model Parameters\n\nI am explaining only those Parameters that I will be implementing below in my function. For an exhaustive explanation of all of them [see here](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n\n**objective [default=reg:linear]**\n\nThis defines the loss function to be minimized. Mostly used values are:\n\n- binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n\n- multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\nyou also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n\n- multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n\n**eval_metric [ default according to objective ]**\n\nThe metric to be used for validation data. The default values are rmse for regression and error for classification.\nTypical values are:\n\n- rmse – root mean square error\n\n- mae – mean absolute error\n\n- logloss – negative log-likelihood\n\n- error – Binary classification error rate (0.5 threshold)\n\n- merror – Multiclass classification error rate\n\n- mlogloss – Multiclass logloss\n\n- auc: Area under the curve\n\n\n**eta [default=0.3]**\n\n- Analogous to learning rate in GBM.\n- Makes the model more robust by shrinking the weights on each step.\n- Typical final values to be used: 0.01-0.2\n\n**colsample_bytree**:  We can create a random sample of the features (or columns) to use prior to creating each decision tree in the boosted model. That is, tuning Column Sub-sampling in XGBoost By Tree. This is controlled by the colsample_bytree parameter. The default value is 1.0 meaning that all columns are used in each decision tree. A fraction (e.g. 0.6) means a fraction of columns to be subsampled. We can evaluate values for colsample_bytree between 0.1 and 1.0 incrementing by 0.1.\n\n#### A Note on regularization in XGBoost\n\nXGBoost adds built-in regularization to achieve accuracy gains beyond gradient boosting. Regularization is the process of adding information to reduce variance and prevent overfitting.\n\nAlthough data may be regularized through hyperparameter fine-tuning, regularized algorithms may also be attempted. For example, Ridge and Lasso are regularized machine learning alternatives to LinearRegression.\n\nXGBoost includes regularization as part of the learning objective, as contrasted with gradient boosting and random forests. The regularized parameters penalize complexity and smooth out the final weights to prevent overfitting. XGBoost is a regularized version of gradient boosting.\n\nMathematically, XGBoost's learning objective may be defined as follows:\n\n### obj(θ) = l(θ) + Ω (θ)\n\nHere, **l(θ)**  is the loss function, which is the Mean Squared Error (MSE) for regression, or the log loss for classification, and **Ω (θ)** is the regularization function, a penalty term to prevent over-fitting. Including a regularization term as part of the objective function distinguishes XGBoost from most tree ensembles.\n\nThe learning objective for the th boosted tree can now be rewritten as follows:\n\n![img](https://i.imgur.com/IRNCrvM.png)\n\n**reg_alpha and reg_lambda** : First note the loss function is defined as\n\n![img](https://i.imgur.com/aw1Hod9.png)\n\n##### So the above is how the regularized objective function looks like if you want to allow for the inclusion of a L1 and a L2 parameter in the same model\n\n`reg_alpha` and `reg_lambda` control the L1 and L2 regularization terms, which in this case limit how extreme the weights at the leaves can become. Higher values of alpha mean more L1 regularization. See the documentation [here](http://xgboost.readthedocs.io/en/latest///parameter.html#parameters-for-tree-booster).\n\nSince L1 regularization in GBDTs is applied to leaf scores rather than directly to features as in logistic regression, it actually serves to reduce the depth of trees. This in turn will tend to reduce the impact of less-predictive features. We might think of L1 regularization as more aggressive against less-predictive features than L2 regularization.\n\nThese two regularization terms have different effects on the weights; L2 regularization (controlled by the lambda term) encourages the weights to be small, whereas L1 regularization (controlled by the alpha term) encourages sparsity — so it encourages weights to go to 0. This is helpful in models such as logistic regression, where you want some feature selection, but in decision trees we’ve already selected our features, so zeroing their weights isn’t super helpful. For this reason, I found setting a high lambda value and a low (or 0) alpha value to be the most effective when regularizing.\n\n##### [From this Paper](https://arxiv.org/pdf/1603.02754.pdf)\n\nYou find the mathematical underpinnings for XGBoost model by Tianqi Chen et al. A couple of mathematical deviations of this model form the classic Friedman's GBM are:\n\n - Regularized (penalized) parameters (and remember that parameters in the boosting are the function, trees, or linear models): L1 and L2 are available.\n\n   [1]: https://i.stack.imgur.com/oJlNo.png","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def xgb_model_run(train_x, train_y, validation_x, validation_y, test_x):\n    params = {\n        'objective': 'reg:squarederror', \n          'eval_metric': 'rmse',\n          'eta': 0.001,\n          'max_depth': 10, \n          'subsample': 0.6, \n          'colsample_bytree': 0.6,\n          'alpha':0.001,\n          'random_state': 42,\n          'tree_method':'gpu_hist'          \n    }\n\n    training_data = xgb.DMatrix(train_x, train_y)\n    validation_data = xgb.DMatrix(validation_x, validation_y)\n\n    watchlist = [(training_data, 'train'), (validation_data, 'valid')]\n\n    model_xgb = xgb.train(params, training_data, 50, watchlist, maximize=False, early_stopping_rounds=100, verbose_eval=100 )\n\n    data_test = xgb.DMatrix(test_x)\n    predict_test_xgb = np.expm1(model_xgb.predict(data_test, ntree_limit=model_xgb.best_ntree_limit ) )\n\n    return predict_test_xgb, model_xgb","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:32:28.941166Z","iopub.execute_input":"2022-04-19T10:32:28.943324Z","iopub.status.idle":"2022-04-19T10:32:28.953890Z","shell.execute_reply.started":"2022-04-19T10:32:28.943287Z","shell.execute_reply":"2022-04-19T10:32:28.952883Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Training XGB","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"predictions_test_y_xgb, model_xgb = xgb_model_run(X_train_split, y_train_split, X_validation, y_validation, X_test_original)\nprint('Completion of XGB Training!!')","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:32:28.957559Z","iopub.execute_input":"2022-04-19T10:32:28.959553Z","iopub.status.idle":"2022-04-19T10:32:39.473271Z","shell.execute_reply.started":"2022-04-19T10:32:28.959514Z","shell.execute_reply":"2022-04-19T10:32:39.472221Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"[0]\ttrain-rmse:14.08767\tvalid-rmse:14.07680\nMultiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n\nWill train until valid-rmse hasn't improved in 100 rounds.\n[49]\ttrain-rmse:13.42463\tvalid-rmse:13.41314\nCompletion of XGB Training!!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Hyper-Parameter Tuning in XGBoost\n\nAs an example, on the above mode, for our XGBoost function we could fine-tune five hyperparameters. The ranges of possible values that we could consider could be as below:\n\n```\n{\"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ] }\n```\n\n---","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"## CatBoost Model Training\n\nCatBoost is another competitor to XGBoost, LightGBM and H2O. “CatBoost” name comes from two words “Category” and “Boosting”.\n\nThe library works well with multiple Categories of data, such as audio, text, image including historical data.\n\nThe CatBoost library can be used to solve both classification and regression challenge. For classification, you can use **“CatBoostClassifier”** and for regression, **“CatBoostRegressor“**.\n\n**[Yandex](https://yandex.com/)** is relying heavily on Catboost for ranking, forecasting and recommendations. This model is serving more than 70 million users each month.\n\n\"CatBoost is an algorithm for gradient boosting on decision trees. Developed by Yandex researchers and engineers, it is the successor of the MatrixNet algorithm that is widely used within the company for ranking tasks, forecasting and making recommendations. It is universal and can be applied across a wide range of areas and to a variety of problems.\"\n\nOverall some of the algorithmic enhancements that **Catboost** brought:\n\n- 1. For data with **categorical** features the accuracy of CatBoost would be better compared to other algorithms.\n\n2. **Better over-fitting handling**: - CatBoost uses the implementation of ordered boosting, an alternative to the classic boosting algorithm, which will be specially significant on small datasets\n\n3. **GPU-training**: - The versions of CatBoost available from pip install (pip install catboost) and conda install (conda install catboost) have GPU support out-of-the-box. You just need to specify that you want to train your model on GPU in the corresponding HP (will be shown below).\n\nFor [GPU system requirements of CatBoost](https://catboost.ai/docs/concepts/python-installation.html#gpu-system-requirements)\n\nThe versions of CatBoost available from pip install and conda install have GPU support out-of-the-box.\nDevices with compute capability 3.0 and higher are supported in compiled packages.\nTraining on GPU requires NVIDIA Driver of version 418.xx or higher.\nThe Python version of CatBoost for CUDA of compute capability 2.0 can be built from source.\n\nTo check Compute Capability of your CUDA-GPU check this [NVIDIA Official Link](https://developer.nvidia.com/cuda-gpus#compute)\n\nFurther for [Training on GPU](https://catboost.ai/docs/features/training-on-gpu.html)\n\nThe parameters that enable and customize training on GPU are set in the constructors of the classes - CatBoost (fit), CatBoostClassifier (fit), CatBoostRegressor (fit).\n`task_type` - The processing unit type to use for training. Possible values are - \"CPU\" or \"GPU\" . An example below\n\n```python\nmodel = CatBoostClassifier(iterations=1000,\n                           task_type=\"GPU\",\n                           devices='0:1')\nmodel.fit(train_data,\n          train_labels,\n          verbose=False)\n```\n\n**Categorical features handling in CatBoost Algorithm**\n\n[The below is taken from this paper](http://learningsys.org/nips17/assets/papers/paper_11.pdf)\n\nCategorical features have a discrete set of values called categories which are not necessary comparable with each other; thus, such features cannot be used in binary decision trees directly. A common practice for dealing with categorical features is converting them to numbers at the preprocessing time, i.e., each category for each example is substituted with one or several numerical values. The most widely used technique which is usually applied to low-cardinality categorical features is one-hot encoding: the original feature is removed and a new binary variable is added for each category [14]. One-hot encoding can be done during the preprocessing phase or during training, the latter can be implemented more efficiently in terms of training time and is implemented in CatBoost.\n\nFor further details on this red [CatBoost's documentation](https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html)\n\n**Leaf growth algorithm in CatBoost**\n\nCatboost grows a balanced tree. In each level of such a tree, the feature-split pair that brings to the lowest loss (according to a penalty function) is selected and is used for all the level’s nodes. It is possible to change its policy using the grow-policy parameter.\n\n## CatBoost Training Parameters\n\nLet’s look at the common parameters in CatBoost:\n\n**loss_function** alias as **objective** — Metric used for training. These are regression metrics such as root mean squared error for regression and logloss for classification.\n\n**eval_metric** — Metric used for detecting over-fitting.\n\n**iterations** — The maximum number of trees to be built, defaults to 1000. It aliases are `num_boost_round`, `n_estimators`, and `num_trees`.\nSome notes on **Total num of Trees** - In **bagging** and **random forests**  the averaging of independently grown trees makes it very difficult to overfit with too many trees. However, in GBMs this function differently as each tree is grown in sequence to fix up the past tree’s mistakes. For example, in regression, GBMs will chase residuals as long as we allow them to. Also, depending on the values of the other hyperparameters, GBMs often require many trees (sometimes many thousands of trees). But also more trees, can easily overfit we must find the optimal number of trees that minimize the loss function of interest with cross validation.\n\n**learning_rate** alias **eta** — The learning rate that determines how fast or slow the model will learn. The default is usually varies between 0.01 to 0.03.\n\n**random_seed** alias **random_state** — The random seed used for training.\n\n**l2_leaf_reg** alias **reg_lambda** — Coefficient at the L2 regularization term of the cost function. The default is 3.0.\n\n**bootstrap_type** — Determines the sampling method for the weights of the objects, e.g Bayesian, Bernoulli, MVS, and Poisson.\ndepth —The depth of the tree.\n\n**grow_policy** — Determines how the greedy search algorithm will be applied. It can be either SymmetricTree, Depthwise, or Lossguide.\n\n**SymmetricTree** is the default. In SymmetricTree, the tree is built level-by-level until the depth is attained. In every step, leaves from the previous tree are split with the same condition. When **Depthwise** is chosen, a tree is built step-by-step until the specified depth is achieved. On each step, all non-terminal leaves from the last tree level are split. The leaves are split using the condition that leads to the best loss improvement. In **Lossguide**, the tree is built leaf-by-leaf until the specified number of leaves is attained. On each step, the non-terminal leaf with the best loss improvement is split\n\n**min_data_in_leaf** alias **min_child_samples** — This is the minimum number of training samples in a leaf. This parameter is only used with the Lossguide and Depthwise growing policies.\n\n**max_leaves** alias num_leaves — This parameter is used only with the Lossguide policy and determines the number of leaves in the tree.\n\n**ignored_features** — Indicates the features that should be ignored in the training process.\n\n**nan_mode** — The method for dealing with missing values. The options are Forbidden, Min, and Max. The default is Min. When Forbidden is used, the presence of missing values leads to errors. With Min, the missing values are taken as the minimum values for that feature. In Max, the missing values are treated as the maximum value for the feature.\n\n**leaf_estimation_method** — The method used to calculate values in leaves. In classification, 10 Newton iterations are used. Regression problems using quantile or MAE loss use one Exact iteration. Multi classification uses one Netwon iteration.\n\n**leaf_estimation_backtracking** — The type of backtracking to be used during gradient descent. The default is `AnyImprovement`. `AnyImprovement` decreases the descent step, up to where the loss function value is smaller than it was in the last iteration. Armijo reduces the descent step until the Armijo condition is met.\n\n**boosting_type** — The boosting scheme. It can be plain for the classic gradient boosting scheme, or ordered, which offers better quality on smaller datasets.\n\n**score_function** — The score type used to select the next split during tree construction. `Cosine` is the default option. The other available options are `L2`, `NewtonL2`, and `NewtonCosine`.\n\n**early_stopping_rounds** — When True, sets the over-fitting detector type to `Iter` and stops the training when the optimal metric is achieved.\n\nclasses_count — The number of classes for multi-classification problems.\ntask_type — Whether you are using a CPU or GPU. CPU is the default.\ndevices — The IDs of the GPU devices to be used for training.\ncat_features — The array with the categorical columns.\ntext_features —Used to declare text columns in classification problems.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# Now Catboost model training\n\nmodel_catboost = CatBoostRegressor(iterations=500,\n                                   learning_rate=0.01,\n                                   depth=10,\n                                   eval_metric='RMSE',\n                                   random_seed = 42,\n                                   bagging_temperature=0.2,\n                                   od_type='Iter',\n                                   metric_period=50,\n                                   od_wait=20\n                                   )\n\nmodel_catboost.fit(X_train_split, y_train_split,\n                   eval_set=(X_validation, y_validation),\n                   use_best_model=True,\n                   verbose=50\n                   )\n\npredictions_test_y_catboost = np.expm1(model_catboost.predict(X_test_original))\n","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:32:39.474818Z","iopub.execute_input":"2022-04-19T10:32:39.475429Z","iopub.status.idle":"2022-04-19T10:59:31.478608Z","shell.execute_reply.started":"2022-04-19T10:32:39.475387Z","shell.execute_reply":"2022-04-19T10:59:31.477796Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n","output_type":"stream"},{"name":"stdout","text":"0:\tlearn: 1.7614936\ttest: 1.6943084\tbest: 1.6943084 (0)\ttotal: 3.39s\tremaining: 28m 12s\n50:\tlearn: 1.6747126\ttest: 1.6330977\tbest: 1.6330977 (50)\ttotal: 2m 44s\tremaining: 24m 12s\n100:\tlearn: 1.6096948\ttest: 1.5902474\tbest: 1.5902474 (100)\ttotal: 5m 26s\tremaining: 21m 30s\n150:\tlearn: 1.5572985\ttest: 1.5589053\tbest: 1.5589053 (150)\ttotal: 8m 7s\tremaining: 18m 45s\n200:\tlearn: 1.5167332\ttest: 1.5382647\tbest: 1.5382647 (200)\ttotal: 10m 47s\tremaining: 16m 3s\n250:\tlearn: 1.4827385\ttest: 1.5217760\tbest: 1.5217760 (250)\ttotal: 13m 28s\tremaining: 13m 22s\n300:\tlearn: 1.4545436\ttest: 1.5093004\tbest: 1.5093004 (300)\ttotal: 16m 9s\tremaining: 10m 40s\n350:\tlearn: 1.4296056\ttest: 1.4994080\tbest: 1.4994080 (350)\ttotal: 18m 49s\tremaining: 7m 59s\n400:\tlearn: 1.4078157\ttest: 1.4916401\tbest: 1.4916401 (400)\ttotal: 21m 30s\tremaining: 5m 18s\n450:\tlearn: 1.3911783\ttest: 1.4861881\tbest: 1.4861881 (450)\ttotal: 24m 12s\tremaining: 2m 37s\n499:\tlearn: 1.3751665\ttest: 1.4807567\tbest: 1.4807567 (499)\ttotal: 26m 49s\tremaining: 0us\n\nbestTest = 1.48075668\nbestIteration = 499\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Creating DataFrame for Final Submission","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"\nsub = pd.read_csv('../input/santander-value-prediction-challenge/sample_submission.csv')\n\nsubmission_lgb = pd.DataFrame()\nsubmission_lgb['target'] = predictions_test_y_light_gbm\n\nsub['target'] = submission_lgb['target']","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:59:31.480078Z","iopub.execute_input":"2022-04-19T10:59:31.480442Z","iopub.status.idle":"2022-04-19T10:59:31.544662Z","shell.execute_reply.started":"2022-04-19T10:59:31.480405Z","shell.execute_reply":"2022-04-19T10:59:31.543975Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## Creating Output file for Submission","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"submission_final = pd.read_csv('../input/santander-value-prediction-challenge/sample_submission.csv')\n\nsubmission_lgb = pd.DataFrame()\nsubmission_lgb['target'] = predictions_test_y_light_gbm\n\n# sub['target'] = submission_lgb['target']\n\nsubmission_xgb = pd.DataFrame()\nsubmission_xgb['target'] = predictions_test_y_xgb\n\nsubmission_catboost = pd.DataFrame()\nsubmission_catboost['target'] = predictions_test_y_catboost\n\nsubmission_final['target'] = (submission_lgb['target'] * 0.5 + submission_xgb['target'] * 0.3 + submission_catboost['target'] * 0.2)","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-04-19T10:59:31.547040Z","iopub.execute_input":"2022-04-19T10:59:31.547547Z","iopub.status.idle":"2022-04-19T10:59:31.598646Z","shell.execute_reply.started":"2022-04-19T10:59:31.547509Z","shell.execute_reply":"2022-04-19T10:59:31.597998Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"submission_final.head()","metadata":{"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"collapsed":false,"execution":{"iopub.status.busy":"2022-04-19T10:59:31.599725Z","iopub.execute_input":"2022-04-19T10:59:31.600047Z","iopub.status.idle":"2022-04-19T10:59:31.609633Z","shell.execute_reply.started":"2022-04-19T10:59:31.600009Z","shell.execute_reply":"2022-04-19T10:59:31.608770Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"          ID        target\n0  000137c73  1.022761e+06\n1  00021489f  1.210934e+06\n2  0004d7953  1.570251e+06\n3  00056a333  2.963400e+06\n4  00056d8eb  1.396908e+06","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000137c73</td>\n      <td>1.022761e+06</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00021489f</td>\n      <td>1.210934e+06</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0004d7953</td>\n      <td>1.570251e+06</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00056a333</td>\n      <td>2.963400e+06</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00056d8eb</td>\n      <td>1.396908e+06</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"submission_final.to_csv('submission_combined_lgb_xgb_catboost.csv', index=False)\n","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-04-19T10:59:31.610997Z","iopub.execute_input":"2022-04-19T10:59:31.611591Z","iopub.status.idle":"2022-04-19T10:59:32.017674Z","shell.execute_reply.started":"2022-04-19T10:59:31.611551Z","shell.execute_reply":"2022-04-19T10:59:32.016859Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Further Reading and References\n\n- T. Chen, C. Guestrin, [XGBoost: A Scalable Tree Boosting System, 2016](https://arxiv.org/abs/1603.02754)\n\n- J. Friedman, [Greedy Function Approximation: A Gradient Boosting Machine 1999](http://docs.salford-systems.com/GreedyFuncApproxSS.pdf)\n\n\n","metadata":{"pycharm":{"name":"#%% md\n"}}}]}